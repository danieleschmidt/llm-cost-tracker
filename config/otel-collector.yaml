receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
  resource:
    attributes:
      - key: service.name
        value: llm-cost-tracker
        action: upsert

exporters:
  # Use OTLP HTTP to send to our custom ingestion service
  otlphttp:
    endpoint: "http://llm-cost-tracker:8000/v1/traces"
    timeout: 10s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    headers:
      "Content-Type": "application/x-protobuf"
  
  prometheus:
    endpoint: "0.0.0.0:8888"
    namespace: llm_cost_tracker
    
  logging:
    loglevel: info

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch, resource]
      exporters: [otlphttp, logging]
    
    metrics:
      receivers: [otlp]
      processors: [batch, resource]
      exporters: [prometheus, logging]

  extensions: []